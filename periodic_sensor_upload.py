import os
import sys
import time
import json
import logging
import argparse
import numpy as np
import requests
from datetime import datetime, timedelta
from pathlib import Path
import torch
from typing import Dict, List, Any, Optional, Tuple
import glob
import pandas as pd

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.absolute()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
from src.data.sensor.sensor_processor import SensorDataProcessor, prepare_sequence_data
from src.models.sensor.lstm_classifier import MultiSensorLSTMClassifier

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join('logs', 'sensor_upload.log'))
    ]
)
logger = logging.getLogger(__name__)

# API ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
API_URL_INSERT = "http://3.34.90.243:8000/vibration-diagnosis/bulk"  # ëŒ€ëŸ‰ ì—…ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸
API_URL_COUNT = "http://3.34.90.243:8000/vibration-diagnosis"  # ì „ì²´ ì§„ë‹¨ ë°ì´í„° ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸

class SensorDataUploader:
    """ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜ ë° ì—…ë¡œë“œ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.data_dir = Path(config.get('data_dir', 'data/raw'))
        self.model_path = Path(config.get('model_path', 'models/sensor_classifier.pth'))
        self.model_info_path = Path(config.get('model_info_path', 'models/model_info.json'))
        self.input_size = config.get('input_size', 4)  # ì„¼ì„œ ìˆ˜
        self.hidden_size = config.get('hidden_size', 64)
        self.num_layers = config.get('num_layers', 2)
        self.num_classes = config.get('num_classes', 4)
        self.sequence_length = config.get('sequence_length', 100)
        self.window_size = config.get('window_size', 15)
        self.device = torch.device(config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))
        self.batch_size = config.get('batch_size', 1000)  # API ì—…ë¡œë“œ ë°°ì¹˜ í¬ê¸°
        
        # ì£¼ê¸° ì„¤ì • (ë¶„ ë‹¨ìœ„)
        self.interval_seconds = config.get('interval_seconds', 0.1) 
        
        # íŒŒì¼ íŒ¨í„´ ì„¤ì •
        self.file_patterns = config.get('file_patterns', ['g2_sensor*_blocks*.csv'])
        
        # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.processor = SensorDataProcessor(
            interpolation_step=config.get('interp_step', 0.001),
            window_size=self.window_size
        )
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = None
        
        # ë¶„ë¥˜ ê²°ê³¼ ë§¤í•‘
        self.class_names = config.get('class_names', ['normal', 'type1', 'type2', 'type3'])
        
        # ë§ˆì§€ë§‰ ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡ (íŒŒì¼ íŒ¨í„´ë³„ë¡œ ê´€ë¦¬)
        self.last_processed_times = {pattern: None for pattern in self.file_patterns}
        
        logger.info("ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜ ë° ì—…ë¡œë“œ í´ë˜ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    def load_model(self) -> bool:
        """
        ëª¨ë¸ ë¡œë“œ
        
        Returns:
            bool: ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_path}")
            
            # ëª¨ë¸ ì •ë³´ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
            if self.model_info_path.exists():
                with open(self.model_info_path, 'r') as f:
                    model_info = json.load(f)
                    self.input_size = model_info.get('input_size', self.input_size)
                    self.hidden_size = model_info.get('hidden_size', self.hidden_size)
                    self.num_layers = model_info.get('num_layers', self.num_layers)
                    self.num_classes = model_info.get('num_classes', self.num_classes)
                    self.sequence_length = model_info.get('sequence_length', self.sequence_length)
                    logger.info(f"ëª¨ë¸ ì •ë³´ ë¡œë“œ: input_size={self.input_size}, hidden_size={self.hidden_size}, sequence_length={self.sequence_length}")
            
            # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            self.model = MultiSensorLSTMClassifier(
                input_size=self.input_size,
                hidden_size=self.hidden_size,
                num_layers=self.num_layers,
                num_classes=self.num_classes
            ).to(self.device)
            
            # ê°€ì¤‘ì¹˜ ë¡œë“œ
            self.model.load_state_dict(torch.load(self.model_path, map_location=self.device))
            self.model.eval()
            
            logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def find_sensor_files(self, pattern: str) -> List[str]:
        """
        íŒ¨í„´ì— ë§ëŠ” ì„¼ì„œ íŒŒì¼ ì°¾ê¸°
        
        Args:
            pattern: íŒŒì¼ íŒ¨í„´ (glob íŒ¨í„´)
            
        Returns:
            List[str]: ì°¾ì€ íŒŒì¼ ê²½ë¡œ ëª©ë¡
        """
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        search_pattern = os.path.join(self.data_dir, pattern)
        files = glob.glob(search_pattern)
        
        # ê° íŒŒì¼ì˜ ìˆ˜ì • ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹  íŒŒì¼ì´ ë¨¼ì €)
        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        
        logger.info(f"íŒ¨í„´ '{pattern}'ìœ¼ë¡œ {len(files)}ê°œ íŒŒì¼ ë°œê²¬: {', '.join(os.path.basename(f) for f in files[:5])}" + 
                   (f" ì™¸ {len(files)-5}ê°œ" if len(files) > 5 else ""))
        
        return files
    
    def process_sensor_file(self, file_path: str) -> Optional[Tuple[np.ndarray, np.ndarray, np.ndarray]]:
        """
        ë‹¨ì¼ ì„¼ì„œ íŒŒì¼ ì²˜ë¦¬
        
        Args:
            file_path: ì„¼ì„œ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            Optional[Tuple[np.ndarray, np.ndarray, np.ndarray]]: (í•™ìŠµ, ê²€ì¦, í…ŒìŠ¤íŠ¸) ë°ì´í„° ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
        """
        try:
            file_name = os.path.basename(file_path)
            logger.info(f"ì„¼ì„œ íŒŒì¼ '{file_name}' ì²˜ë¦¬ ì¤‘...")
            
            # CSV íŒŒì¼ ë¡œë“œ
            df = pd.read_csv(file_path, names=["time", "normal", "type1", "type2", "type3"], header=None)
            
            # ë°ì´í„° ê²€ì¦
            if not all(col in df.columns for col in ["time", "normal", "type1", "type2", "type3"]):
                logger.error(f"í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                return None
            
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°ì´í„° ë³€í™˜ (SensorDataProcessorì™€ í˜¸í™˜ë˜ë„ë¡)
            sensor_id = self._extract_sensor_id(file_name)  # íŒŒì¼ ì´ë¦„ì—ì„œ ì„¼ì„œ ID ì¶”ì¶œ
            interpolated_data = {
                f'sensor{sensor_id}': df
            }
            
            # ë°ì´í„° ê²°í•© ë° ì „ì²˜ë¦¬
            logger.info("ì„¼ì„œ ë°ì´í„° ê²°í•© ë° ì „ì²˜ë¦¬ ì¤‘...")
            processed_data = self.processor.combine_and_preprocess_sensor_data(interpolated_data)
            
            # ë°ì´í„° ë¶„í• 
            train_data, valid_data, test_data = self.processor.split_and_combine_data(
                processed_data,
                train_ratio=0.6,
                valid_ratio=0.2,
                test_ratio=0.2
            )
            
            logger.info(f"ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: í•™ìŠµ={train_data.shape}, ê²€ì¦={valid_data.shape}, í…ŒìŠ¤íŠ¸={test_data.shape}")
            return train_data, valid_data, test_data
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None
    
    def _extract_sensor_id(self, file_name: str) -> int:
        """
        íŒŒì¼ ì´ë¦„ì—ì„œ ì„¼ì„œ ID ì¶”ì¶œ
        
        Args:
            file_name: íŒŒì¼ ì´ë¦„ (ì˜ˆ: g2_sensor1_blocks.csv)
            
        Returns:
            int: ì„¼ì„œ ID (ê¸°ë³¸ê°’ 1)
        """
        try:
            # g2_sensor1_blocks.csv í˜•ì‹ì—ì„œ '1' ì¶”ì¶œ
            import re
            match = re.search(r'sensor(\d+)_', file_name)
            if match:
                return int(match.group(1))
            return 1  # ê¸°ë³¸ê°’
        except:
            return 1  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’
    
    def classify_sensor_data(self, sensor_data: np.ndarray) -> List[Dict[str, Any]]:
        """
        ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜ ë° ê²°ê³¼ ìƒì„±
        
        Args:
            sensor_data: ì„¼ì„œ ë°ì´í„° ë°°ì—´
            
        Returns:
            List[Dict[str, Any]]: ë¶„ë¥˜ ê²°ê³¼ ëª©ë¡
        """
        if self.model is None:
            logger.error("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            # ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
            logger.info("ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
            X_data, y_data = prepare_sequence_data(sensor_data, sequence_length=self.sequence_length)
            
            logger.info(f"ì‹œí€€ìŠ¤ ë°ì´í„° í˜•íƒœ: {X_data.shape}")
            
            # ë¶„ë¥˜ ê²°ê³¼ ëª©ë¡
            classification_results = []
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            batch_size = 32  # ëª¨ë¸ ì¶”ë¡  ë°°ì¹˜ í¬ê¸°
            num_samples = X_data.shape[0]
            
            for i in range(0, num_samples, batch_size):
                end_idx = min(i + batch_size, num_samples)
                batch_data = X_data[i:end_idx]
                
                # í…ì„œ ë³€í™˜
                batch_tensor = torch.tensor(batch_data, dtype=torch.float32).to(self.device)
                
                # ì˜ˆì¸¡ ìˆ˜í–‰
                with torch.no_grad():
                    outputs = self.model(batch_tensor)
                    probabilities = torch.nn.functional.softmax(outputs, dim=1)
                    confidence, predicted = torch.max(probabilities, 1)
                
                # ê²°ê³¼ ë³€í™˜
                for j in range(batch_data.shape[0]):
                    data_idx = i + j
                    pred_class = predicted[j].item()
                    conf_value = confidence[j].item()
                    
                    # ì‹¤ì œ ë ˆì´ë¸”ì´ ìˆìœ¼ë©´ í•¨ê»˜ ì €ì¥
                    actual_class = y_data[data_idx] if y_data is not None else None
                    
                    # í´ë˜ìŠ¤ëª… ë§¤í•‘
                    predicted_label = self.class_names[pred_class] if pred_class < len(self.class_names) else f"unknown_{pred_class}"
                    
                    # ê²°ê³¼ ì €ì¥
                    classification_results.append({
                        "sequence_data": batch_data[j].tolist(),  # ì‹œí€€ìŠ¤ ë°ì´í„° ì €ì¥
                        "predicted_class": int(pred_class),  # ì˜ˆì¸¡ í´ë˜ìŠ¤ (ì •ìˆ˜)
                        "predicted_label": predicted_label,  # ì˜ˆì¸¡ í´ë˜ìŠ¤ëª…
                        "confidence": float(conf_value),  # ì‹ ë¢°ë„
                        "actual_class": int(actual_class) if actual_class is not None else None,  # ì‹¤ì œ í´ë˜ìŠ¤ (ì •ìˆ˜, ìˆëŠ” ê²½ìš°)
                        "timestamp": datetime.now().isoformat()  # íƒ€ì„ìŠ¤íƒ¬í”„
                    })
            
            logger.info(f"ë¶„ë¥˜ ì™„ë£Œ: {len(classification_results)}ê°œ ê²°ê³¼ ìƒì„±")
            return classification_results
            
        except Exception as e:
            logger.error(f"ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    def format_for_api(self, machine_name: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        API ì—…ë¡œë“œìš©ìœ¼ë¡œ ê²°ê³¼ í˜•ì‹ ë³€í™˜
        
        Args:
            machine_name: ê¸°ê³„ ì´ë¦„ (íŒŒì¼ ì´ë¦„ì—ì„œ ì¶”ì¶œ)
            results: ë¶„ë¥˜ ê²°ê³¼ ëª©ë¡
            
        Returns:
            List[Dict[str, Any]]: API í˜•ì‹ ë°ì´í„°
        """
        api_data = []
        
        for result in results:
            # fault_type ë³€í™˜: 0=normal, 1,2,3=ê³ ì¥ ìœ í˜•
            fault_type = 0 if result["predicted_class"] == 0 else result["predicted_class"]
            
            # ì›ì‹œ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
            sequence_data_json = json.dumps({
                "sequence": result["sequence_data"],
                "confidence": result["confidence"],
                "actual_class": result["actual_class"]
            })
            
            # API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            api_data.append({
                "machine_name": machine_name,
                "detected_at": result["timestamp"],
                "fault_type": fault_type,
                "sensor_data": sequence_data_json  # ì„¼ì„œ ë°ì´í„°ë¥¼ ì¶”ê°€ í•„ë“œë¡œ ì „ì†¡
            })
        
        return api_data
    
    def upload_to_api(self, api_data: List[Dict[str, Any]]) -> int:
        """
        ë¶„ë¥˜ ê²°ê³¼ë¥¼ API ì„œë²„ë¡œ ì—…ë¡œë“œ
        
        Args:
            api_data: ì—…ë¡œë“œí•  API í˜•ì‹ ë°ì´í„°
            
        Returns:
            int: ì—…ë¡œë“œëœ ë ˆì½”ë“œ ìˆ˜
        """
        if not api_data:
            logger.warning("ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        total_uploaded = 0
        
        try:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in range(0, len(api_data), self.batch_size):
                batch = api_data[i:i+self.batch_size]
                
                # API ìš”ì²­
                response = requests.post(API_URL_INSERT, json=batch)
                
                if response.status_code == 200:
                    total_uploaded += len(batch)
                    logger.info(f"âœ… ëˆ„ì  {total_uploaded}ê°œ ì—…ë¡œë“œ ì™„ë£Œ: {response.json()}")
                else:
                    logger.error(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨ ({i} ~ {i+len(batch)}): {response.status_code} {response.text}")
                    break
            
            return total_uploaded
            
        except Exception as e:
            logger.error(f"API ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return total_uploaded
    
    def check_total_count(self) -> int:
        """
        API ì„œë²„ ë‚´ ì „ì²´ ë°ì´í„° ìˆ˜ í™•ì¸
        
        Returns:
            int: ì „ì²´ ë°ì´í„° ìˆ˜
        """
        try:
            response = requests.get(API_URL_COUNT)
            response.raise_for_status()
            total = len(response.json())
            logger.info(f"ğŸ“Š í˜„ì¬ ì§„ë‹¨ ë°ì´í„° ì´ {total}ê±´ ì¡´ì¬í•©ë‹ˆë‹¤.")
            return total
        except Exception as e:
            logger.error(f"âŒ ì „ì²´ ë°ì´í„° ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return -1

    def process_single_cycle(self) -> int:
        """
        ë‹¨ì¼ ì£¼ê¸° ì²˜ë¦¬ (ëª¨ë“  ì„¼ì„œì— ëŒ€í•´ í•œ ë²ˆ ì²˜ë¦¬)
        
        Returns:
            int: ì—…ë¡œë“œëœ ì´ ë ˆì½”ë“œ ìˆ˜
        """
        # í˜„ì¬ API ì„œë²„ ë°ì´í„° ìˆ˜ í™•ì¸
        initial_count = self.check_total_count()
        
        total_results = 0
        
        # ê° íŒŒì¼ íŒ¨í„´ ì²˜ë¦¬
        for pattern in self.file_patterns:
            logger.info(f"\n=== íŒ¨í„´ '{pattern}' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ===")
            
            # íŒ¨í„´ì— ë§ëŠ” íŒŒì¼ ì°¾ê¸°
            files = self.find_sensor_files(pattern)
            
            if not files:
                logger.warning(f"íŒ¨í„´ '{pattern}'ì— ë§ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ê° íŒŒì¼ ì²˜ë¦¬ (ìµœì‹  íŒŒì¼ì„ ìš°ì„  ì²˜ë¦¬)
            for file_path in files[:5]:  # ìµœì‹  íŒŒì¼ 5ê°œë§Œ ì²˜ë¦¬
                file_name = os.path.basename(file_path)
                
                logger.info(f"\n--- íŒŒì¼ '{file_name}' ì²˜ë¦¬ ì¤‘ ---")
                
                # ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬
                result = self.process_sensor_file(file_path)
                if result is None:
                    logger.warning(f"íŒŒì¼ '{file_name}' ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    continue
                
                train_data, valid_data, test_data = result
                
                # í•™ìŠµ ë°ì´í„°ë§Œ ìƒ˜í”Œë§í•˜ì—¬ ì‚¬ìš© (ì „ì²´ ë°ì´í„°ë¥¼ ë‹¤ ì˜¬ë¦¬ë©´ ë„ˆë¬´ ë§ì„ ìˆ˜ ìˆìŒ)
                # ì¶”ê°€: ë§¤ë²ˆ ë‹¤ë¥¸ ìƒ˜í”Œì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ëœë¤ ì‹œë“œ ë³€ê²½
                np.random.seed(int(time.time()))
                
                if train_data.shape[0] > 1000:
                    sample_indices = np.random.choice(train_data.shape[0], 1000, replace=False)
                    sample_data = train_data[sample_indices]
                    logger.info(f"{file_name} í•™ìŠµ ë°ì´í„°ì—ì„œ 1000ê°œ ìƒ˜í”Œ ì¶”ì¶œ: {sample_data.shape}")
                else:
                    sample_data = train_data
                
                # ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜
                classification_results = self.classify_sensor_data(sample_data)
                if not classification_results:
                    logger.warning(f"{file_name} ë¶„ë¥˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # íŒŒì¼ ì´ë¦„ì—ì„œ ê¸°ê³„ ì´ë¦„ ì¶”ì¶œ (g2_sensor1_blocks.csv -> g2_sensor1)
                machine_name = os.path.splitext(file_name)[0]
                if machine_name.endswith('_blocks'):
                    machine_name = machine_name[:-7]  # '_blocks' ì œê±°
                
                # API ì „ì†¡ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                api_data = self.format_for_api(machine_name, classification_results)
                
                # API ì„œë²„ë¡œ ì—…ë¡œë“œ
                uploaded = self.upload_to_api(api_data)
                total_results += uploaded
                
                logger.info(f"{file_name} ì²˜ë¦¬ ì™„ë£Œ: {uploaded}ê°œ ê²°ê³¼ ì—…ë¡œë“œ")
            
            # ë§ˆì§€ë§‰ ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            self.last_processed_times[pattern] = datetime.now()
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        final_count = self.check_total_count()
        
        logger.info("\n=== ì²˜ë¦¬ ì£¼ê¸° ì™„ë£Œ ===")
        logger.info(f"- ì´ˆê¸° ë°ì´í„° ìˆ˜: {initial_count}")
        logger.info(f"- ì—…ë¡œë“œëœ ê²°ê³¼ ìˆ˜: {total_results}")
        logger.info(f"- ìµœì¢… ë°ì´í„° ìˆ˜: {final_count}")
        
        if final_count >= 0:
            difference = final_count - initial_count
            if difference != total_results:
                logger.warning(f"ì—…ë¡œë“œëœ ê²°ê³¼ ìˆ˜ ({total_results})ì™€ ì‹¤ì œ ì¦ê°€í•œ ë°ì´í„° ìˆ˜ ({difference})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
        return total_results
    
    def run_periodic(self, max_cycles: int = -1) -> None:
        """
        ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        
        Args:
            max_cycles: ìµœëŒ€ ì‹¤í–‰ ì£¼ê¸° ìˆ˜ (-1ì€ ë¬´í•œ ë°˜ë³µ)
        """
        # ëª¨ë¸ ë¡œë“œ
        if not self.load_model():
            logger.error("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        logger.info(f"ì£¼ê¸°ì  ì‹¤í–‰ ì‹œì‘: ê°„ê²© {self.interval_seconds}ë¶„, ìµœëŒ€ ì£¼ê¸° {max_cycles if max_cycles > 0 else 'ë¬´í•œ'}")
        
        try:
            cycle_count = 0
            
            while max_cycles < 0 or cycle_count < max_cycles:
                cycle_start_time = time.time()
                cycle_count += 1
                
                logger.info(f"\n===== ì£¼ê¸° {cycle_count} ì‹œì‘ =====")
                logger.info(f"í˜„ì¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                
                # ë‹¨ì¼ ì£¼ê¸° ì²˜ë¦¬
                total_uploaded = self.process_single_cycle()
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                logger.info(f"ì£¼ê¸° {cycle_count} ì™„ë£Œ: {total_uploaded}ê°œ ë ˆì½”ë“œ ì—…ë¡œë“œë¨")
                
                # ë‹¤ìŒ ì£¼ê¸°ê¹Œì§€ ëŒ€ê¸° (ì²˜ë¦¬ ì‹œê°„ ê³ ë ¤)
                cycle_end_time = time.time()
                cycle_duration = cycle_end_time - cycle_start_time
                
                wait_time = (self.interval_seconds) - cycle_duration
                if wait_time > 0:
                    logger.info(f"ë‹¤ìŒ ì£¼ê¸°ê¹Œì§€ {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(wait_time)
                else:
                    logger.warning(f"ì£¼ê¸° ì²˜ë¦¬ì— {cycle_duration:.1f}ì´ˆ ì†Œìš”, ê°„ê²©({self.interval_seconds * 60}ì´ˆ)ë³´ë‹¤ ê¸¸ì–´ ì¦‰ì‹œ ë‹¤ìŒ ì£¼ê¸° ì‹œì‘")
                
        except KeyboardInterrupt:
            logger.info("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ì£¼ê¸°ì  ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        finally:
            logger.info("ì£¼ê¸°ì  ì‹¤í–‰ ì¢…ë£Œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='ì£¼ê¸°ì  ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜ ë° API ì„œë²„ ì—…ë¡œë“œ')
    
    parser.add_argument('--data_dir', type=str, default='data/raw',
                      help='ì„¼ì„œ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--model_path', type=str, default='models/sensor_classifier.pth',
                      help='ëª¨ë¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--model_info_path', type=str, default='models/model_info.json',
                      help='ëª¨ë¸ ì •ë³´ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--sequence_length', type=int, default=50,
                      help='ì‹œí€€ìŠ¤ ê¸¸ì´')
    parser.add_argument('--batch_size', type=int, default=1000,
                      help='API ì—…ë¡œë“œ ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--file_patterns', type=str, nargs='+', default=['g2_sensor*_blocks*.csv'],
                      help='ì²˜ë¦¬í•  ì„¼ì„œ íŒŒì¼ íŒ¨í„´ ëª©ë¡ (glob íŒ¨í„´)')
    parser.add_argument('--interval_seconds', type=int, default=30,
                      help='ì²˜ë¦¬ ì£¼ê¸° (ë¶„ ë‹¨ìœ„)')
    parser.add_argument('--max_cycles', type=int, default=-1,
                      help='ìµœëŒ€ ì‹¤í–‰ ì£¼ê¸° ìˆ˜ (-1ì€ ë¬´í•œ ë°˜ë³µ)')
    
    args = parser.parse_args()
    
    # ì„¤ì • ë”•ì…”ë„ˆë¦¬ ìƒì„±
    config = vars(args)
    
    # ì—…ë¡œë” ìƒì„± ë° ì£¼ê¸°ì  ì‹¤í–‰
    uploader = SensorDataUploader(config)
    uploader.run_periodic(max_cycles=args.max_cycles)


if __name__ == "__main__":
    main()