#!/usr/bin/env python
"""
ì£¼ê¸°ì  ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜ ë° ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. ì¼ì • ê°„ê²©ìœ¼ë¡œ ì„¼ì„œ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
2. ì €ì¥ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜
3. ë¶„ë¥˜ ê²°ê³¼ ë° ì„¼ì„œ ë°ì´í„°ë¥¼ API ì„œë²„ì— ì—…ë¡œë“œ
4. ì„¤ì •ëœ ê°„ê²©ìœ¼ë¡œ ì£¼ê¸°ì ìœ¼ë¡œ ë°˜ë³µ ì‹¤í–‰
"""

import os
import sys
import time
import json
import logging
import argparse
import numpy as np
import requests
from datetime import datetime, timedelta
from pathlib import Path
import torch
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.absolute()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
from src.models.sensor.lstm_classifier import MultiSensorLSTMClassifier

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join('logs', 'sensor_upload.log'))
    ]
)
logger = logging.getLogger(__name__)

# API ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
API_URL_INSERT = "http://3.34.90.243:8000/vibration-diagnosis/bulk"  # ëŒ€ëŸ‰ ì—…ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸
API_URL_COUNT = "http://3.34.90.243:8000/vibration-diagnosis"  # ì „ì²´ ì§„ë‹¨ ë°ì´í„° ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸

class SensorDataUploader:
    """ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜ ë° ì—…ë¡œë“œ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.data_dir = Path(config.get('data_dir', 'data/raw'))
        self.model_path = Path(config.get('model_path', 'models/sensor_classifier.pth'))
        self.model_info_path = Path(config.get('model_info_path', 'models/model_info.json'))
        self.input_size = config.get('input_size', 4)  # ì„¼ì„œ ìˆ˜
        self.hidden_size = config.get('hidden_size', 64)
        self.num_layers = config.get('num_layers', 2)
        self.num_classes = config.get('num_classes', 4)
        self.sequence_length = config.get('sequence_length', 50)
        self.window_size = config.get('window_size', 15)
        self.device = torch.device(config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))
        self.batch_size = config.get('batch_size', 1000)  # API ì—…ë¡œë“œ ë°°ì¹˜ í¬ê¸°
        
        # ì£¼ê¸° ì„¤ì • (ì´ˆ ë‹¨ìœ„)
        self.interval_minutes = config.get('interval_minutes', 60) 
        
        # ì—¬ëŸ¬ ì„¼ì„œ íŒŒì¼ ì ‘ë‘ì‚¬ ì§€ì • (g1, g2, g3, g4, g5)
        self.file_prefixes = config.get('file_prefixes', ['g1', 'g2'])
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = None
        
        # ë¶„ë¥˜ ê²°ê³¼ ë§¤í•‘
        self.class_names = config.get('class_names', ['normal', 'type1', 'type2', 'type3'])
        
        # ë§ˆì§€ë§‰ ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
        self.last_processed_times = {prefix: None for prefix in self.file_prefixes}
        
        logger.info("ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜ ë° ì—…ë¡œë“œ í´ë˜ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    def load_model(self) -> bool:
        """
        ëª¨ë¸ ë¡œë“œ
        
        Returns:
            bool: ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_path}")
            
            # ëª¨ë¸ ì •ë³´ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
            if self.model_info_path.exists():
                with open(self.model_info_path, 'r') as f:
                    model_info = json.load(f)
                    self.input_size = model_info.get('input_size', self.input_size)
                    self.hidden_size = model_info.get('hidden_size', self.hidden_size)
                    self.num_layers = model_info.get('num_layers', self.num_layers)
                    self.num_classes = model_info.get('num_classes', self.num_classes)
                    self.sequence_length = model_info.get('sequence_length', self.sequence_length)
                    logger.info(f"ëª¨ë¸ ì •ë³´ ë¡œë“œ: input_size={self.input_size}, hidden_size={self.hidden_size}, sequence_length={self.sequence_length}")
            
            # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            self.model = MultiSensorLSTMClassifier(
                input_size=self.input_size,
                hidden_size=self.hidden_size,
                num_layers=self.num_layers,
                num_classes=self.num_classes
            ).to(self.device)
            
            # ê°€ì¤‘ì¹˜ ë¡œë“œ
            self.model.load_state_dict(torch.load(self.model_path, map_location=self.device))
            self.model.eval()
            
            logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def process_sensor_data(self, file_prefix: str) -> Optional[np.ndarray]:
        """
        ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬
        
        Args:
            file_prefix: ì²˜ë¦¬í•  ì„¼ì„œ íŒŒì¼ ì ‘ë‘ì‚¬ (g1, g2 ë“±)
            
        Returns:
            Optional[np.ndarray]: ì²˜ë¦¬ëœ ë°ì´í„° ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
        """
        try:
            # ì„¼ì„œ íŒŒì¼ ê²½ë¡œ êµ¬ì„± (4ê°œ ì„¼ì„œ)
            sensor_files = [
                f"{file_prefix}_sensor1_blocks.csv",
                f"{file_prefix}_sensor2_blocks.csv",
                f"{file_prefix}_sensor3_blocks.csv",
                f"{file_prefix}_sensor4_blocks.csv"
            ]
            
            # ê° ì„¼ì„œ ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬
            sensor_data = {}
            
            for i, sensor_file in enumerate(sensor_files, start=1):
                file_path = os.path.join(self.data_dir, sensor_file)
                
                if not os.path.exists(file_path):
                    logger.warning(f"ì„¼ì„œ {i}ì˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                    continue
                
                # ë°ì´í„° ë¡œë“œ
                try:
                    # 'time'ê³¼ 'value' ì»¬ëŸ¼ì„ ê°€ì§„ CSV íŒŒì¼ ë¡œë“œ
                    df = pd.read_csv(file_path)
                    
                    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
                    if 'time' not in df.columns or 'value' not in df.columns:
                        # ì»¬ëŸ¼ ì´ë¦„ì´ ë‹¤ë¥´ë©´ ê¸°ë³¸ ì´ë¦„ìœ¼ë¡œ ê°€ì •
                        if len(df.columns) >= 2:
                            df.columns = ['time', 'value']
                        else:
                            logger.error(f"{file_path} íŒŒì¼ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                            continue
                    
                    # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
                    df = df.sort_values(by='time')
                    
                    # ë°ì´í„° ì •ê·œí™” (í•„ìš”í•œ ê²½ìš°)
                    # MinMax ìŠ¤ì¼€ì¼ë§ ì ìš© (-1 ~ 1 ë²”ìœ„)
                    value_col = df['value'].values
                    min_val = value_col.min()
                    max_val = value_col.max()
                    
                    if max_val > min_val:
                        normalized_data = -1 + 2 * (value_col - min_val) / (max_val - min_val)
                    else:
                        normalized_data = np.zeros_like(value_col)
                    
                    # ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
                    sensor_data[f'sensor{i}'] = normalized_data
                    logger.info(f"ì„¼ì„œ {i} ë°ì´í„° ë¡œë“œ ë° ì •ê·œí™” ì™„ë£Œ: {len(normalized_data)} ìƒ˜í”Œ")
                    
                except Exception as e:
                    logger.error(f"ì„¼ì„œ {i} ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    continue
            
            # ëª¨ë“  ì„¼ì„œê°€ ì—†ëŠ” ê²½ìš°
            if not sensor_data:
                logger.error(f"ì²˜ë¦¬í•  ì„¼ì„œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {file_prefix}")
                return None
            
            # ëª¨ë“  ì„¼ì„œ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë§ì¶¤
            min_length = min(len(data) for data in sensor_data.values())
            aligned_data = {}
            
            for sensor_name, data in sensor_data.items():
                aligned_data[sensor_name] = data[:min_length]
            
            # ì„¼ì„œ ë°ì´í„° ê²°í•© (ì—´ë°©í–¥)
            combined_data = np.column_stack([aligned_data[f'sensor{i}'] for i in range(1, 5) if f'sensor{i}' in aligned_data])
            
            # ì—†ëŠ” ì„¼ì„œê°€ ìˆëŠ” ê²½ìš° ë¹ˆ ì—´ ì¶”ê°€
            missing_sensors = 4 - combined_data.shape[1]
            if missing_sensors > 0:
                padding = np.zeros((combined_data.shape[0], missing_sensors))
                combined_data = np.hstack((combined_data, padding))
                logger.warning(f"{missing_sensors}ê°œ ì„¼ì„œ ë°ì´í„°ê°€ ì—†ì–´ 0ìœ¼ë¡œ ì±„ì› ìŠµë‹ˆë‹¤.")
            
            logger.info(f"ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: í˜•íƒœ={combined_data.shape}")
            return combined_data
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None
        
    def classify_sensor_data(self, sensor_data: np.ndarray, interval: float = 1.0) -> List[Dict[str, Any]]:
        """
        ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜ ë° ê²°ê³¼ ìƒì„± - 1ì´ˆë§ˆë‹¤ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì‚¬ìš©
        
        Args:
            sensor_data: ì„¼ì„œ ë°ì´í„° ë°°ì—´
            interval: ë¶„ë¥˜ ê°„ê²© (ì´ˆ ë‹¨ìœ„), ê¸°ë³¸ê°’ 1.0ì´ˆ
            
        Returns:
            List[Dict[str, Any]]: API í˜•ì‹ ë¶„ë¥˜ ê²°ê³¼ ëª©ë¡
        """
        if self.model is None:
            logger.error("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        window_size = self.sequence_length
        
        if len(sensor_data) < window_size:
            logger.error(f"ë°ì´í„° ê¸¸ì´({len(sensor_data)})ê°€ ìœˆë„ìš° í¬ê¸°({window_size})ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤.")
            return []
        
        # API í˜•ì‹ ê²°ê³¼ ëª©ë¡ (ë°”ë¡œ API í˜•ì‹ìœ¼ë¡œ ìƒì„±)
        api_results = []
        
        try:
            # ìœˆë„ìš° ìˆ˜ ê³„ì‚° (ìŠ¤í… í¬ê¸° 1ë¡œ ê³ ì •)
            step = 200
            num_windows = (len(sensor_data) - window_size) // step + 1
            logger.info(f"ì²˜ë¦¬í•  ìœˆë„ìš° ìˆ˜: {num_windows} (ê°„ê²©: {interval}ì´ˆ)")
            
            for i in range(0, num_windows):
                start_time = time.time()
                
                # í˜„ì¬ ìœˆë„ìš° ì¶”ì¶œ
                start_idx = i * step
                end_idx = start_idx + window_size
                window = sensor_data[start_idx:end_idx]
                
                # ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
                model_input = torch.tensor(window, dtype=torch.float32).unsqueeze(0).to(self.device)
                
                # ì˜ˆì¸¡ ìˆ˜í–‰
                with torch.no_grad():
                    outputs = self.model(model_input)
                    probabilities = torch.nn.functional.softmax(outputs, dim=1)
                    confidence, predicted = torch.max(probabilities, 1)
                
                # ê²°ê³¼ ì¶”ì¶œ
                pred_class = predicted.item()
                conf_value = confidence.item()
                
                # í´ë˜ìŠ¤ëª… ë§¤í•‘
                predicted_label = self.class_names[pred_class] if pred_class < len(self.class_names) else f"unknown_{pred_class}"
                
                # API í˜•ì‹ìœ¼ë¡œ ë°”ë¡œ ê²°ê³¼ ìƒì„±
                api_result = {
                    "predicted_class": int(pred_class),
                    "predicted_label": predicted_label,
                    "confidence": float(conf_value),
                    "timestamp": datetime.now().isoformat()
                }
                api_results.append(api_result)
                
                # ë‹¤ìŒ ìœˆë„ìš° ì²˜ë¦¬ ì „ì— ì¼ì • ì‹œê°„ ëŒ€ê¸°
                elapsed = time.time() - start_time
                if elapsed < interval:
                    time.sleep(interval - elapsed)
                else:
                    logger.warning(f"ì²˜ë¦¬ ì‹œê°„ì´ ê°„ê²©ë³´ë‹¤ ê¹ë‹ˆë‹¤: {elapsed:.4f}ì´ˆ > {interval}ì´ˆ")
            
            logger.info(f"ë¶„ë¥˜ ì™„ë£Œ: ì´ {len(api_results)}ê°œ ìœˆë„ìš° ì²˜ë¦¬ë¨")
            return api_results
        
        except Exception as e:
            logger.error(f"ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []

    def format_for_api(self, machine_name: str, classification_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        API ì—…ë¡œë“œìš©ìœ¼ë¡œ ê²°ê³¼ í˜•ì‹ ë³€í™˜
        
        Args:
            machine_name: ê¸°ê³„ ì´ë¦„ (g1, g2 ë“±)
            classification_results: ë¶„ë¥˜ ê²°ê³¼ ëª©ë¡
            
        Returns:
            List[Dict[str, Any]]: API í˜•ì‹ ë°ì´í„°
        """
        api_data = []
        
        for result in classification_results:
            # fault_type ë³€í™˜: 0=normal, 1,2,3=ê³ ì¥ ìœ í˜•
            fault_type = 0 if result["predicted_class"] == 0 else result["predicted_class"]
            
            # API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            api_data.append({
                "machine_name": machine_name,
                "detected_at": result["timestamp"],
                "fault_type": fault_type
            })
        
        return api_data
    
    def upload_to_api(self, api_data: List[Dict[str, Any]]) -> int:
        """
        ë¶„ë¥˜ ê²°ê³¼ë¥¼ API ì„œë²„ë¡œ ì—…ë¡œë“œ
        
        Args:
            api_data: ì—…ë¡œë“œí•  API í˜•ì‹ ë°ì´í„°
            
        Returns:
            int: ì—…ë¡œë“œëœ ë ˆì½”ë“œ ìˆ˜
        """
        if not api_data:
            logger.warning("ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        total_uploaded = 0
        
        try:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in range(0, len(api_data), self.batch_size):
                batch = api_data[i:i+self.batch_size]
                
                # API ìš”ì²­
                response = requests.post(API_URL_INSERT, json=batch)
                
                if response.status_code == 200:
                    total_uploaded += len(batch)
                    logger.info(f"âœ… ëˆ„ì  {total_uploaded}ê°œ ì—…ë¡œë“œ ì™„ë£Œ: {response.json()}")
                else:
                    logger.error(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨ ({i} ~ {i+len(batch)}): {response.status_code} {response.text}")
                    break
            
            return total_uploaded
            
        except Exception as e:
            logger.error(f"API ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return total_uploaded
    
    def check_total_count(self) -> int:
        """
        API ì„œë²„ ë‚´ ì „ì²´ ë°ì´í„° ìˆ˜ í™•ì¸
        
        Returns:
            int: ì „ì²´ ë°ì´í„° ìˆ˜
        """
        try:
            response = requests.get(API_URL_COUNT)
            response.raise_for_status()
            total = len(response.json())
            logger.info(f"ğŸ“Š í˜„ì¬ ì§„ë‹¨ ë°ì´í„° ì´ {total}ê±´ ì¡´ì¬í•©ë‹ˆë‹¤.")
            return total
        except Exception as e:
            logger.error(f"âŒ ì „ì²´ ë°ì´í„° ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return -1

    def process_single_cycle(self) -> int:
        """
        ë‹¨ì¼ ì£¼ê¸° ì²˜ë¦¬ (ëª¨ë“  ì„¼ì„œì— ëŒ€í•´ í•œ ë²ˆ ì²˜ë¦¬)
        
        Returns:
            int: ì—…ë¡œë“œëœ ì´ ë ˆì½”ë“œ ìˆ˜
        """
        # í˜„ì¬ API ì„œë²„ ë°ì´í„° ìˆ˜ í™•ì¸
        initial_count = self.check_total_count()
        
        total_results = 0
        
        # ê° ì„¼ì„œ íŒŒì¼ ì²˜ë¦¬
        for file_prefix in self.file_prefixes:
            logger.info(f"\n=== {file_prefix} ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ ===")
            
            # ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬
            processed_data = self.process_sensor_data(file_prefix)
            if processed_data is None:
                logger.warning(f"{file_prefix} ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            # ë°ì´í„° ìƒ˜í”Œë§ (ì „ì²´ ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì€ ê²½ìš°)
            if len(processed_data) > 10000:  # ìµœëŒ€ 10,000 ìƒ˜í”Œ ì²˜ë¦¬
                # ëœë¤ ì‹œë“œ ì„¤ì • (ë§¤ë²ˆ ë‹¤ë¥¸ ìƒ˜í”Œì„ ìœ„í•´)
                np.random.seed(int(time.time()))
                sample_indices = np.random.choice(len(processed_data), 10000, replace=False)
                sample_data = processed_data[sample_indices]
                logger.info(f"{file_prefix} ë°ì´í„°ì—ì„œ 10000ê°œ ìƒ˜í”Œ ì¶”ì¶œ: {sample_data.shape}")
            else:
                sample_data = processed_data
            
            # ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜
            classification_results = self.classify_sensor_data(sample_data)
            if not classification_results:
                logger.warning(f"{file_prefix} ë¶„ë¥˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # API ì „ì†¡ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            api_data = self.format_for_api(file_prefix, classification_results)
            
            # API ì„œë²„ë¡œ ì—…ë¡œë“œ
            uploaded = self.upload_to_api(api_data)
            total_results += uploaded
            
            # ë§ˆì§€ë§‰ ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            self.last_processed_times[file_prefix] = datetime.now()
            logger.info(f"{file_prefix} ì²˜ë¦¬ ì™„ë£Œ: {uploaded}ê°œ ê²°ê³¼ ì—…ë¡œë“œ")
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        final_count = self.check_total_count()
        
        logger.info("\n=== ì²˜ë¦¬ ì£¼ê¸° ì™„ë£Œ ===")
        logger.info(f"- ì´ˆê¸° ë°ì´í„° ìˆ˜: {initial_count}")
        logger.info(f"- ì—…ë¡œë“œëœ ê²°ê³¼ ìˆ˜: {total_results}")
        logger.info(f"- ìµœì¢… ë°ì´í„° ìˆ˜: {final_count}")
        
        if final_count >= 0:
            difference = final_count - initial_count
            if difference != total_results:
                logger.warning(f"ì—…ë¡œë“œëœ ê²°ê³¼ ìˆ˜ ({total_results})ì™€ ì‹¤ì œ ì¦ê°€í•œ ë°ì´í„° ìˆ˜ ({difference})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
        return total_results
    
    def run_periodic(self, max_cycles: int = -1) -> None:
        """
        ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        
        Args:
            max_cycles: ìµœëŒ€ ì‹¤í–‰ ì£¼ê¸° ìˆ˜ (-1ì€ ë¬´í•œ ë°˜ë³µ)
        """
        # ëª¨ë¸ ë¡œë“œ
        if not self.load_model():
            logger.error("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        logger.info(f"ì£¼ê¸°ì  ì‹¤í–‰ ì‹œì‘: ê°„ê²© {self.interval_minutes}ì´ˆ, ìµœëŒ€ ì£¼ê¸° {max_cycles if max_cycles > 0 else 'ë¬´í•œ'}")
        
        try:
            cycle_count = 0
            
            while max_cycles < 0 or cycle_count < max_cycles:
                cycle_start_time = time.time()
                cycle_count += 1
                
                logger.info(f"\n===== ì£¼ê¸° {cycle_count} ì‹œì‘ =====")
                logger.info(f"í˜„ì¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                
                # ë‹¨ì¼ ì£¼ê¸° ì²˜ë¦¬
                total_uploaded = self.process_single_cycle()
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                logger.info(f"ì£¼ê¸° {cycle_count} ì™„ë£Œ: {total_uploaded}ê°œ ë ˆì½”ë“œ ì—…ë¡œë“œë¨")
                
                # ë‹¤ìŒ ì£¼ê¸°ê¹Œì§€ ëŒ€ê¸° (ì²˜ë¦¬ ì‹œê°„ ê³ ë ¤)
                cycle_end_time = time.time()
                cycle_duration = cycle_end_time - cycle_start_time
                
                wait_time = (self.interval_minutes) - cycle_duration
                if wait_time > 0:
                    logger.info(f"ë‹¤ìŒ ì£¼ê¸°ê¹Œì§€ {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(wait_time)
                else:
                    logger.warning(f"ì£¼ê¸° ì²˜ë¦¬ì— {cycle_duration:.1f}ì´ˆ ì†Œìš”, ê°„ê²©({wait_time}ì´ˆ)ë³´ë‹¤ ê¸¸ì–´ ì¦‰ì‹œ ë‹¤ìŒ ì£¼ê¸° ì‹œì‘")
                
        except KeyboardInterrupt:
            logger.info("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ì£¼ê¸°ì  ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        finally:
            logger.info("ì£¼ê¸°ì  ì‹¤í–‰ ì¢…ë£Œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='ì£¼ê¸°ì  ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜ ë° API ì„œë²„ ì—…ë¡œë“œ')
    
    parser.add_argument('--data_dir', type=str, default='data/blocks',
                      help='ì„¼ì„œ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--model_path', type=str, default='models/sensor_classifier.pth',
                      help='ëª¨ë¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--model_info_path', type=str, default='models/model_info.json',
                      help='ëª¨ë¸ ì •ë³´ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--sequence_length', type=int, default=50,
                      help='ì‹œí€€ìŠ¤ ê¸¸ì´')
    parser.add_argument('--batch_size', type=int, default=1000,
                      help='API ì—…ë¡œë“œ ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--file_prefixes', type=str, nargs='+', default=['g1', 'g2'],
                      help='ì²˜ë¦¬í•  ì„¼ì„œ íŒŒì¼ ì ‘ë‘ì‚¬ ëª©ë¡')
    parser.add_argument('--interval_minutes', type=int, default=60,
                      help='ì²˜ë¦¬ ì£¼ê¸° (ë¶„ ë‹¨ìœ„)')
    parser.add_argument('--max_cycles', type=int, default=-1,
                      help='ìµœëŒ€ ì‹¤í–‰ ì£¼ê¸° ìˆ˜ (-1ì€ ë¬´í•œ ë°˜ë³µ)')
    
    args = parser.parse_args()
    
    # ì„¤ì • ë”•ì…”ë„ˆë¦¬ ìƒì„±
    config = vars(args)
    
    # ì—…ë¡œë” ìƒì„± ë° ì£¼ê¸°ì  ì‹¤í–‰
    uploader = SensorDataUploader(config)
    uploader.run_periodic(max_cycles=args.max_cycles)


if __name__ == "__main__":
    main()