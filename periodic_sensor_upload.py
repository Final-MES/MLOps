#!/usr/bin/env python
"""
ì£¼ê¸°ì  ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜ ë° ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. ì¼ì • ê°„ê²©ìœ¼ë¡œ ì„¼ì„œ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
2. ì €ì¥ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜
3. ë¶„ë¥˜ ê²°ê³¼ ë° ì„¼ì„œ ë°ì´í„°ë¥¼ API ì„œë²„ì— ì—…ë¡œë“œ
4. ì„¤ì •ëœ ê°„ê²©ìœ¼ë¡œ ì£¼ê¸°ì ìœ¼ë¡œ ë°˜ë³µ ì‹¤í–‰
"""

import os
import sys
import time
import json
import logging
import argparse
import numpy as np
import requests
from datetime import datetime, timedelta
from pathlib import Path
import torch
from typing import Dict, List, Any, Optional, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.absolute()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
from src.data.sensor.sensor_processor import SensorDataProcessor, prepare_sequence_data
from src.models.sensor.lstm_classifier import MultiSensorLSTMClassifier

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join('logs', 'sensor_upload.log'))
    ]
)
logger = logging.getLogger(__name__)

# API ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
API_URL_INSERT = "http://3.34.90.243:8000/vibration-diagnosis/bulk"  # ëŒ€ëŸ‰ ì—…ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸
API_URL_COUNT = "http://3.34.90.243:8000/vibration-diagnosis"  # ì „ì²´ ì§„ë‹¨ ë°ì´í„° ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸

class SensorDataUploader:
    """ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜ ë° ì—…ë¡œë“œ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.data_dir = Path(config.get('data_dir', 'data/raw'))
        self.model_path = Path(config.get('model_path', 'models/sensor_classifier.pth'))
        self.model_info_path = Path(config.get('model_info_path', 'models/model_info.json'))
        self.input_size = config.get('input_size', 4)  # ì„¼ì„œ ìˆ˜
        self.hidden_size = config.get('hidden_size', 64)
        self.num_layers = config.get('num_layers', 2)
        self.num_classes = config.get('num_classes', 4)
        self.sequence_length = config.get('sequence_length', 50)
        self.window_size = config.get('window_size', 15)
        self.device = torch.device(config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))
        self.batch_size = config.get('batch_size', 1000)  # API ì—…ë¡œë“œ ë°°ì¹˜ í¬ê¸°
        
        # ì£¼ê¸° ì„¤ì • (ë¶„ ë‹¨ìœ„)
        self.interval_minutes = config.get('interval_minutes', 30) 
        
        # ì—¬ëŸ¬ ì„¼ì„œ íŒŒì¼ ì ‘ë‘ì‚¬ ì§€ì • (g1, g2, g3, g4, g5)
        self.file_prefixes = config.get('file_prefixes', ['g1', 'g2', 'g3', 'g4', 'g5'])
        
        # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.processor = SensorDataProcessor(
            interpolation_step=config.get('interp_step', 0.001),
            window_size=self.window_size
        )
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = None
        
        # ë¶„ë¥˜ ê²°ê³¼ ë§¤í•‘
        self.class_names = config.get('class_names', ['normal', 'type1', 'type2', 'type3'])
        
        # ë§ˆì§€ë§‰ ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
        self.last_processed_times = {prefix: None for prefix in self.file_prefixes}
        
        logger.info("ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜ ë° ì—…ë¡œë“œ í´ë˜ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    def load_model(self) -> bool:
        """
        ëª¨ë¸ ë¡œë“œ
        
        Returns:
            bool: ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_path}")
            
            # ëª¨ë¸ ì •ë³´ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
            if self.model_info_path.exists():
                with open(self.model_info_path, 'r') as f:
                    model_info = json.load(f)
                    self.input_size = model_info.get('input_size', self.input_size)
                    self.hidden_size = model_info.get('hidden_size', self.hidden_size)
                    self.num_layers = model_info.get('num_layers', self.num_layers)
                    self.num_classes = model_info.get('num_classes', self.num_classes)
                    self.sequence_length = model_info.get('sequence_length', self.sequence_length)
                    logger.info(f"ëª¨ë¸ ì •ë³´ ë¡œë“œ: input_size={self.input_size}, hidden_size={self.hidden_size}, sequence_length={self.sequence_length}")
            
            # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            self.model = MultiSensorLSTMClassifier(
                input_size=self.input_size,
                hidden_size=self.hidden_size,
                num_layers=self.num_layers,
                num_classes=self.num_classes
            ).to(self.device)
            
            # ê°€ì¤‘ì¹˜ ë¡œë“œ
            self.model.load_state_dict(torch.load(self.model_path, map_location=self.device))
            self.model.eval()
            
            logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def process_sensor_data(self, file_prefix: str) -> Optional[Tuple[np.ndarray, np.ndarray, np.ndarray]]:
        """
        ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬
        
        Args:
            file_prefix: ì²˜ë¦¬í•  ì„¼ì„œ íŒŒì¼ ì ‘ë‘ì‚¬ (g1, g2 ë“±)
            
        Returns:
            Optional[Tuple[np.ndarray, np.ndarray, np.ndarray]]: (í•™ìŠµ, ê²€ì¦, í…ŒìŠ¤íŠ¸) ë°ì´í„° ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
        """
        try:
            # ì„¼ì„œ íŒŒì¼ ë¡œë“œ ë° ë³´ê°„
            logger.info(f"{file_prefix} ì„¼ì„œ ë°ì´í„° ë¡œë“œ ë° ë³´ê°„ ì¤‘...")
            interpolated_data = self.processor.load_and_interpolate_sensor_data(
                self.data_dir, prefix=file_prefix
            )
            
            if not interpolated_data:
                logger.error(f"{file_prefix} ì„¼ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ë°ì´í„° ê²°í•© ë° ì „ì²˜ë¦¬
            logger.info("ì„¼ì„œ ë°ì´í„° ê²°í•© ë° ì „ì²˜ë¦¬ ì¤‘...")
            processed_data = self.processor.combine_and_preprocess_sensor_data(interpolated_data)
            
            # ë°ì´í„° ë¶„í• 
            train_data, valid_data, test_data = self.processor.split_and_combine_data(
                processed_data,
                train_ratio=0.6,
                valid_ratio=0.2,
                test_ratio=0.2
            )
            
            logger.info(f"ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: í•™ìŠµ={train_data.shape}, ê²€ì¦={valid_data.shape}, í…ŒìŠ¤íŠ¸={test_data.shape}")
            return train_data, valid_data, test_data
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None
    
    def classify_sensor_data(self, sensor_data: np.ndarray) -> List[Dict[str, Any]]:
        """
        ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜ ë° ê²°ê³¼ ìƒì„±
        
        Args:
            sensor_data: ì„¼ì„œ ë°ì´í„° ë°°ì—´
            
        Returns:
            List[Dict[str, Any]]: ë¶„ë¥˜ ê²°ê³¼ ëª©ë¡
        """
        if self.model is None:
            logger.error("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            # ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
            logger.info("ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
            X_data, y_data = prepare_sequence_data(sensor_data, sequence_length=self.sequence_length)
            
            logger.info(f"ì‹œí€€ìŠ¤ ë°ì´í„° í˜•íƒœ: {X_data.shape}")
            
            # ë¶„ë¥˜ ê²°ê³¼ ëª©ë¡
            classification_results = []
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            batch_size = 32  # ëª¨ë¸ ì¶”ë¡  ë°°ì¹˜ í¬ê¸°
            num_samples = X_data.shape[0]
            
            for i in range(0, num_samples, batch_size):
                end_idx = min(i + batch_size, num_samples)
                batch_data = X_data[i:end_idx]
                
                # í…ì„œ ë³€í™˜
                batch_tensor = torch.tensor(batch_data, dtype=torch.float32).to(self.device)
                
                # ì˜ˆì¸¡ ìˆ˜í–‰
                with torch.no_grad():
                    outputs = self.model(batch_tensor)
                    probabilities = torch.nn.functional.softmax(outputs, dim=1)
                    confidence, predicted = torch.max(probabilities, 1)
                
                # ê²°ê³¼ ë³€í™˜
                for j in range(batch_data.shape[0]):
                    data_idx = i + j
                    pred_class = predicted[j].item()
                    conf_value = confidence[j].item()
                    
                    # ì‹¤ì œ ë ˆì´ë¸”ì´ ìˆìœ¼ë©´ í•¨ê»˜ ì €ì¥
                    actual_class = y_data[data_idx] if y_data is not None else None
                    
                    # í´ë˜ìŠ¤ëª… ë§¤í•‘
                    predicted_label = self.class_names[pred_class] if pred_class < len(self.class_names) else f"unknown_{pred_class}"
                    
                    # ê²°ê³¼ ì €ì¥
                    classification_results.append({
                        "sequence_data": batch_data[j].tolist(),  # ì‹œí€€ìŠ¤ ë°ì´í„° ì €ì¥
                        "predicted_class": int(pred_class),  # ì˜ˆì¸¡ í´ë˜ìŠ¤ (ì •ìˆ˜)
                        "predicted_label": predicted_label,  # ì˜ˆì¸¡ í´ë˜ìŠ¤ëª…
                        "confidence": float(conf_value),  # ì‹ ë¢°ë„
                        "actual_class": int(actual_class) if actual_class is not None else None,  # ì‹¤ì œ í´ë˜ìŠ¤ (ì •ìˆ˜, ìˆëŠ” ê²½ìš°)
                        "timestamp": datetime.now().isoformat()  # íƒ€ì„ìŠ¤íƒ¬í”„
                    })
            
            logger.info(f"ë¶„ë¥˜ ì™„ë£Œ: {len(classification_results)}ê°œ ê²°ê³¼ ìƒì„±")
            return classification_results
            
        except Exception as e:
            logger.error(f"ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    def format_for_api(self, machine_name: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        API ì—…ë¡œë“œìš©ìœ¼ë¡œ ê²°ê³¼ í˜•ì‹ ë³€í™˜
        
        Args:
            machine_name: ê¸°ê³„ ì´ë¦„ (g1, g2 ë“±)
            results: ë¶„ë¥˜ ê²°ê³¼ ëª©ë¡
            
        Returns:
            List[Dict[str, Any]]: API í˜•ì‹ ë°ì´í„°
        """
        api_data = []
        
        for result in results:
            # fault_type ë³€í™˜: 0=normal, 1,2,3=ê³ ì¥ ìœ í˜•
            fault_type = 0 if result["predicted_class"] == 0 else result["predicted_class"]
            
            # ì›ì‹œ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
            sequence_data_json = json.dumps({
                "sequence": result["sequence_data"],
                "confidence": result["confidence"],
                "actual_class": result["actual_class"]
            })
            
            # API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            api_data.append({
                "machine_name": machine_name,
                "detected_at": result["timestamp"],
                "fault_type": fault_type,
                "sensor_data": sequence_data_json  # ì„¼ì„œ ë°ì´í„°ë¥¼ ì¶”ê°€ í•„ë“œë¡œ ì „ì†¡
            })
        
        return api_data
    
    def upload_to_api(self, api_data: List[Dict[str, Any]]) -> int:
        """
        ë¶„ë¥˜ ê²°ê³¼ë¥¼ API ì„œë²„ë¡œ ì—…ë¡œë“œ
        
        Args:
            api_data: ì—…ë¡œë“œí•  API í˜•ì‹ ë°ì´í„°
            
        Returns:
            int: ì—…ë¡œë“œëœ ë ˆì½”ë“œ ìˆ˜
        """
        if not api_data:
            logger.warning("ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        total_uploaded = 0
        
        try:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in range(0, len(api_data), self.batch_size):
                batch = api_data[i:i+self.batch_size]
                
                # API ìš”ì²­
                response = requests.post(API_URL_INSERT, json=batch)
                
                if response.status_code == 200:
                    total_uploaded += len(batch)
                    logger.info(f"âœ… ëˆ„ì  {total_uploaded}ê°œ ì—…ë¡œë“œ ì™„ë£Œ: {response.json()}")
                else:
                    logger.error(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨ ({i} ~ {i+len(batch)}): {response.status_code} {response.text}")
                    break
            
            return total_uploaded
            
        except Exception as e:
            logger.error(f"API ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return total_uploaded
    
    def check_total_count(self) -> int:
        """
        API ì„œë²„ ë‚´ ì „ì²´ ë°ì´í„° ìˆ˜ í™•ì¸
        
        Returns:
            int: ì „ì²´ ë°ì´í„° ìˆ˜
        """
        try:
            response = requests.get(API_URL_COUNT)
            response.raise_for_status()
            total = len(response.json())
            logger.info(f"ğŸ“Š í˜„ì¬ ì§„ë‹¨ ë°ì´í„° ì´ {total}ê±´ ì¡´ì¬í•©ë‹ˆë‹¤.")
            return total
        except Exception as e:
            logger.error(f"âŒ ì „ì²´ ë°ì´í„° ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return -1

    def process_single_cycle(self) -> int:
        """
        ë‹¨ì¼ ì£¼ê¸° ì²˜ë¦¬ (ëª¨ë“  ì„¼ì„œì— ëŒ€í•´ í•œ ë²ˆ ì²˜ë¦¬)
        
        Returns:
            int: ì—…ë¡œë“œëœ ì´ ë ˆì½”ë“œ ìˆ˜
        """
        # í˜„ì¬ API ì„œë²„ ë°ì´í„° ìˆ˜ í™•ì¸
        initial_count = self.check_total_count()
        
        total_results = 0
        
        # ê° ì„¼ì„œ íŒŒì¼ ì²˜ë¦¬
        for file_prefix in self.file_prefixes:
            logger.info(f"\n=== {file_prefix} ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ ===")
            
            # ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬
            result = self.process_sensor_data(file_prefix)
            if result is None:
                logger.warning(f"{file_prefix} ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            train_data, valid_data, test_data = result
            
            # í•™ìŠµ ë°ì´í„°ë§Œ ìƒ˜í”Œë§í•˜ì—¬ ì‚¬ìš© (ì „ì²´ ë°ì´í„°ë¥¼ ë‹¤ ì˜¬ë¦¬ë©´ ë„ˆë¬´ ë§ì„ ìˆ˜ ìˆìŒ)
            # ì¶”ê°€: ë§¤ë²ˆ ë‹¤ë¥¸ ìƒ˜í”Œì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ëœë¤ ì‹œë“œ ë³€ê²½
            np.random.seed(int(time.time()))
            
            if train_data.shape[0] > 1000:
                sample_indices = np.random.choice(train_data.shape[0], 1000, replace=False)
                sample_data = train_data[sample_indices]
                logger.info(f"{file_prefix} í•™ìŠµ ë°ì´í„°ì—ì„œ 1000ê°œ ìƒ˜í”Œ ì¶”ì¶œ: {sample_data.shape}")
            else:
                sample_data = train_data
            
            # ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜
            classification_results = self.classify_sensor_data(sample_data)
            if not classification_results:
                logger.warning(f"{file_prefix} ë¶„ë¥˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # API ì „ì†¡ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            api_data = self.format_for_api(file_prefix, classification_results)
            
            # API ì„œë²„ë¡œ ì—…ë¡œë“œ
            uploaded = self.upload_to_api(api_data)
            total_results += uploaded
            
            # ë§ˆì§€ë§‰ ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            self.last_processed_times[file_prefix] = datetime.now()
            logger.info(f"{file_prefix} ì²˜ë¦¬ ì™„ë£Œ: {uploaded}ê°œ ê²°ê³¼ ì—…ë¡œë“œ")
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        final_count = self.check_total_count()
        
        logger.info("\n=== ì²˜ë¦¬ ì£¼ê¸° ì™„ë£Œ ===")
        logger.info(f"- ì´ˆê¸° ë°ì´í„° ìˆ˜: {initial_count}")
        logger.info(f"- ì—…ë¡œë“œëœ ê²°ê³¼ ìˆ˜: {total_results}")
        logger.info(f"- ìµœì¢… ë°ì´í„° ìˆ˜: {final_count}")
        
        if final_count >= 0:
            difference = final_count - initial_count
            if difference != total_results:
                logger.warning(f"ì—…ë¡œë“œëœ ê²°ê³¼ ìˆ˜ ({total_results})ì™€ ì‹¤ì œ ì¦ê°€í•œ ë°ì´í„° ìˆ˜ ({difference})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
        return total_results
    
    def run_periodic(self, max_cycles: int = -1) -> None:
        """
        ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        
        Args:
            max_cycles: ìµœëŒ€ ì‹¤í–‰ ì£¼ê¸° ìˆ˜ (-1ì€ ë¬´í•œ ë°˜ë³µ)
        """
        # ëª¨ë¸ ë¡œë“œ
        if not self.load_model():
            logger.error("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        logger.info(f"ì£¼ê¸°ì  ì‹¤í–‰ ì‹œì‘: ê°„ê²© {self.interval_minutes}ë¶„, ìµœëŒ€ ì£¼ê¸° {max_cycles if max_cycles > 0 else 'ë¬´í•œ'}")
        
        try:
            cycle_count = 0
            
            while max_cycles < 0 or cycle_count < max_cycles:
                cycle_start_time = time.time()
                cycle_count += 1
                
                logger.info(f"\n===== ì£¼ê¸° {cycle_count} ì‹œì‘ =====")
                logger.info(f"í˜„ì¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                
                # ë‹¨ì¼ ì£¼ê¸° ì²˜ë¦¬
                total_uploaded = self.process_single_cycle()
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                logger.info(f"ì£¼ê¸° {cycle_count} ì™„ë£Œ: {total_uploaded}ê°œ ë ˆì½”ë“œ ì—…ë¡œë“œë¨")
                
                # ë‹¤ìŒ ì£¼ê¸°ê¹Œì§€ ëŒ€ê¸° (ì²˜ë¦¬ ì‹œê°„ ê³ ë ¤)
                cycle_end_time = time.time()
                cycle_duration = cycle_end_time - cycle_start_time
                
                wait_time = (self.interval_minutes * 60) - cycle_duration
                if wait_time > 0:
                    logger.info(f"ë‹¤ìŒ ì£¼ê¸°ê¹Œì§€ {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(wait_time)
                else:
                    logger.warning(f"ì£¼ê¸° ì²˜ë¦¬ì— {cycle_duration:.1f}ì´ˆ ì†Œìš”, ê°„ê²©({self.interval_minutes * 60}ì´ˆ)ë³´ë‹¤ ê¸¸ì–´ ì¦‰ì‹œ ë‹¤ìŒ ì£¼ê¸° ì‹œì‘")
                
        except KeyboardInterrupt:
            logger.info("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ì£¼ê¸°ì  ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        finally:
            logger.info("ì£¼ê¸°ì  ì‹¤í–‰ ì¢…ë£Œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='ì£¼ê¸°ì  ì„¼ì„œ ë°ì´í„° ë¶„ë¥˜ ë° API ì„œë²„ ì—…ë¡œë“œ')
    
    parser.add_argument('--data_dir', type=str, default='data/raw',
                      help='ì„¼ì„œ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--model_path', type=str, default='models/sensor_classifier.pth',
                      help='ëª¨ë¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--model_info_path', type=str, default='models/model_info.json',
                      help='ëª¨ë¸ ì •ë³´ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--sequence_length', type=int, default=50,
                      help='ì‹œí€€ìŠ¤ ê¸¸ì´')
    parser.add_argument('--batch_size', type=int, default=1000,
                      help='API ì—…ë¡œë“œ ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--file_prefixes', type=str, nargs='+', default=['g1', 'g2', 'g3', 'g4', 'g5'],
                      help='ì²˜ë¦¬í•  ì„¼ì„œ íŒŒì¼ ì ‘ë‘ì‚¬ ëª©ë¡')
    parser.add_argument('--interval_minutes', type=int, default=30,
                      help='ì²˜ë¦¬ ì£¼ê¸° (ë¶„ ë‹¨ìœ„)')
    parser.add_argument('--max_cycles', type=int, default=-1,
                      help='ìµœëŒ€ ì‹¤í–‰ ì£¼ê¸° ìˆ˜ (-1ì€ ë¬´í•œ ë°˜ë³µ)')
    
    args = parser.parse_args()
    
    # ì„¤ì • ë”•ì…”ë„ˆë¦¬ ìƒì„±
    config = vars(args)
    
    # ì—…ë¡œë” ìƒì„± ë° ì£¼ê¸°ì  ì‹¤í–‰
    uploader = SensorDataUploader(config)
    uploader.run_periodic(max_cycles=args.max_cycles)


if __name__ == "__main__":
    main()